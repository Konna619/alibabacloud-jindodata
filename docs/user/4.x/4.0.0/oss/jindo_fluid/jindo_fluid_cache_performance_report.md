# AI 训练场景 JindoFS 缓存加速最佳实践

## 概述
如今 AI 训练已成为大数据场景下的一个重要的业务场景，大数据场景下数据一般由统一的存储系统进行维护，例如搭建分布式文件系统 HDFS，抑或是数据湖方案下使用对象存储。
AI 训练一般会使用独立的集群，配备诸如 GPU 等硬件资源以获得更高的计算性能，这样 AI 训练作业就需要从大数据存储系统上读取源数据，因此数据读取性能势必成为了影响训练作业整体执行效率的重要因素。
JindoFS 提供的缓存功能能够对多种常用的大数据存储系统（HDFS、对象存储OSS等）实现缓存，并且对 AI 训练场景有针对性的优化，能够做为 AI 训练场景的数据加速利器，本文将详细阐述 JindoFS 实现 AI 训练场景数据加速的最佳实践。

## JindoFS 针对 AI 训练场景的优化支持
### 数据和元数据缓存
AI 训练场景通常会额外配备诸如 GPU 等硬件资源提升计算性能，这就意味着需要与之匹配的 IO 性能才能真正提升整体作业执行效率，而且 AI 训练往往需要读取海量的训练数据，因此数据读取性能也就成为了该场景下的一个关键性因素。
而直接读取存储在远端 HDFS 集群或者对象存储上的数据往往很难满足训练场景所需要的 IO 性能，JindoFS 可以利用训练集群上的磁盘以及内存资源，搭建一个集群内的分布式缓存系统，支持对目标数据源的元数据和数据进行缓存，能够提供高吞吐、低延时的 IO 性能，以满足 AI 训练的需求。

### 缓存预加载
开启缓存后，元数据和数据默认都会在第一次访问数据的时候缓存到 JindoFS 分布式缓存系统中，然而受限于 AI 训练作业的读取模式以及数据规模，靠训练作业自己冷启动读取数据往往不是一种经济高效的做法，因此 JindoFS 也提供了缓存预加载命令，可以以较高的效率预先将训练所需的数据目录的元数据和数据加载到本地的分布式缓存系统中，这样训练作业就可以直接热启动，访问缓存数据，避免作业再从后端存储上拉取数据。

### 小文件优化
AI 训练的数据集往往会包含大量小文件（例如大量的图片文件），小文件的读取性能也是一些常见大数据存储系统的一个痛点，大量小文件意味着对读取延时特别敏感，从一个远端的大数据存储系统上读取，延时上往往会成为瓶颈，另外对于存储系统的元数据服务（例如 HDFS 的 NameNode）也会产生较大的冲击，在这种场景下，缓存就显得更为重要。
而且 JindoFS 也对小文件的数据组织管理上进行了针对性的优化，能够提供高效的小文件访问性能，因此对于 AI 训练场景具有更好的适配性。

### Fuse 支持
JindoFS 在大数据分析场景下主要提供了兼容 Hadoop 文件系统的文件访问接口，而 AI 训练场景的数据读取则是完全不同的访问方式，传统的单机 AI 训练往往是直接读取本地文件，因此 JindoFS 提供了 fuse 支持，使得传统的 AI 训练作业也能够非常容易地对接大数据存储系统。
另外 JindoFS 也提供了 python 接口等多样的访问接口，可以很好地对接各种常用的 AI 训练场景的使用方式。

## 实践及性能分析
本文针对两种典型的大数据存储系统（对象存储 OSS 和 HDFS）进行性能测试及分析，使用 InsightFace 作为测试用例（关于 InsightFace 可详见 [拥抱云原生，Fluid结合JindoFS：加速机器学习训练-InsightFace](jindo_fluid_insightFace_example.md)），该测试集为典型的小文件训练集，包含约380万个小文件，每个文件大小约为23KB。测试集群配置如下。

|  |  |
| --- | --- |
| 实例数 | 3 |
| 实例规格 | gn6v-c10g1.20xlarge |
| 硬件配置 | 82 vCPU, 336 GiB 内存, 8 V100 GPU |
| 磁盘 | ESSD 500GB * 1 |

部署3个节点的 JindoFS 分布式缓存，使用本地磁盘进行数据缓存，并将 JindoFS 挂载为 fuse 作为训练作业的数据源。
测试过程首先执行元数据和数据预加载命令，将目标目录的数据和元数据缓存到 JindoFS 缓存系统中，然后执行 InsightFace 作业，作为对比，将 JindoFS 的缓存开关关闭后再次执行，则不利用缓存，所有元数据和数据均直接访问后端存储系统。

### 加速对象存储 OSS
数据存储在阿里云对象存储 OSS 上，利用 JindoFS 进行缓存加速（关于如何使用 JindoFS 加速 OSS 可参考文档 [拥抱云原生，Fluid结合JindoFS ：阿里云OSS加速利器](jindo_fluid_jindofs_oss_introduce.md)），结果如下所示：

| 元数据预加载 | 数据预加载 | 开启缓存 | 关闭缓存 |
| --- | --- | --- |--- |
| 6min | 6min | 41min | 11小时进度约17%，总耗时预计>100小时|

在没有缓存的情况下，直接访问 OSS，作业基本无法在合理的时间内完成，这主要是因为对象存储的单个请求访问延时较高，对于海量小文件，若作业本身没有很大的并发，每个文件的访问延时（包括元数据访问延时以及数据读取延时）会成为整个作业的瓶颈。
而 JindoFS 元数据和数据预加载命令通过高并发同步 OSS 数据，可以在较短的时间内完成元数据和数据预加载，后续的作业即可利用缓存，访问延时上能够获得数量级上的提升。

### 加速 HDFS
数据存储在另一个 HDFS 集群上，利用 JindoFS 进行缓存加速（关于如何使用 JindoFS 加速 HDFS 可参考文档 [拥抱云原生，Fluid结合JindoFS：加速 HDFS 使用指南](jindo_fluid_jindofs_hdfs_introduce.md)），HDFS 集群配置如下：

|  |  |
| --- | --- |
| 实例数 | 2 NameNode (HA) + 3 DataNode |
| 实例规格 | ecs.c6.4xlarge |
| 硬件配置 | 16 vCPU, 32 GiB 内存 |
| 磁盘 | ESSD 500GB * 4 |

相关测试结果如下所示：

| 元数据预加载 | 数据预加载 | 开启缓存 | 关闭缓存 | 关闭缓存（模拟 HDFS 集群繁忙） |
| --- | --- | --- |--- | --- |
| 1min18s | 5min | 40min | 44min | 55min |

测试结果显示对 HDFS 数据进行缓存之后能够获得一定的性能提升，不过提升幅度不大（约 10%），这个主要因为测试规模较小，HDFS 集群也是测试用集群，本身比较空闲，因此 HDFS 集群本身的延时和吞吐能力并没有明显受限，导致缓存带来的提升有限。另外我们也模拟了 HDFS 集群繁忙的情况，在 HDFS 集群上同步运行 Terasort 作业，可以看到训练作业执行效率明显受到影响，在这种情况下缓存可以带来近 30% 的性能提升，在实际生产使用的 HDFS 集群上缓存效果可能会更加明显。

下面再对 HDFS 集群的相关监控指标进行分析，首先观察 DataNode 节点，下图所示分别是关闭缓存和开启缓存情况下某个 DataNode 节点的网络吞吐情况，缓存之后 DataNode 吞吐几乎降为零，可以大幅减小对 HDFS 集群 DataNode 的网络带宽压力，在实际生产集群上，HDFS 集群往往会有大量的大数据分析作业，无论是 DataNode 读取数据还是作业 shuffle 过程，都会对节点的磁盘和网络带宽产生巨大压力，从而对 AI 训练作业的数据读取产生明显的影响，通过 JindoFS 缓存可以很好的减轻 DataNode 的带宽压力。
* 关闭缓存：
<img src="../pic/jindofs_fluid_cache_performance_report_1.png" alt="title" width="700"/>

* 开启缓存：
<img src="../pic/jindofs_fluid_cache_performance_report_2.png" alt="title" width="700"/>

下面是 NameNode 节点的网络吞吐及 CPU 监控，可以看到大量小文件访问对 NameNode 的网络及 CPU 也会产生显著的压力，另外NameNode 作为一个单点服务，大量小文件访问也很容易对其稳定性产生影响，通过 JindoFS 进行元数据缓存后可以有效降低对 NameNode 的冲击。
* 关闭缓存：
<img src="../pic/jindofs_fluid_cache_performance_report_3.png" alt="title" width="700"/>
<img src="../pic/jindofs_fluid_cache_performance_report_4.png" alt="title" width="700"/>

* 开启缓存：
<img src="../pic/jindofs_fluid_cache_performance_report_5.png" alt="title" width="700"/>
<img src="../pic/jindofs_fluid_cache_performance_report_6.png" alt="title" width="700"/>

## 总结
AI 训练场景对于数据读取有很高的性能要求，而且海量的小文件对于访问延时也非常敏感，通过 JindoFS 的缓存能力可以有效地对大数据存储系统上的数据进行缓存加速，提供稳定可靠的高吞吐、低延时的数据访问性能，同时也可以有效地缓解对后端存储系统的的压力，保证后端存储的稳定性。
